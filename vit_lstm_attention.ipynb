{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Collecting pycocoevalcap\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap) (2.0.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
      "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "!python -m nltk.downloader punkt\n",
    "\n",
    "!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Mechanism (Bahdanau Attention)\n",
    "    Giúp Decoder \"nhìn\" vào các vùng quan trọng của ảnh khi sinh từng từ.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # Biến đổi feature ảnh\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # Biến đổi trạng thái hidden LSTM\n",
    "        self.full_att = nn.Linear(attention_dim, 1)               # Tính điểm năng lượng\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        # encoder_out: (Batch, Num_Pixels, Encoder_Dim) -> (B, 197, 768)\n",
    "        # decoder_hidden: (Batch, Decoder_Dim) -> (B, 512)\n",
    "\n",
    "        att1 = self.encoder_att(encoder_out)          # (B, 197, Att_Dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)       # (B, Att_Dim)\n",
    "\n",
    "        # Cộng broadcast: (B, 197, Att_Dim) + (B, 1, Att_Dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))) # (B, 197, 1)\n",
    "        \n",
    "        alpha = self.softmax(att)                     # (B, 197, 1) - Trọng số sự chú ý\n",
    "        attention_weighted_encoding = (encoder_out * alpha).sum(dim=1) # (B, Encoder_Dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class ViT_LSTM_Attention(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_idx, embed_dim=512, hidden_dim=512, attention_dim=256, unfreeze_layers=2, dropout=0.3):\n",
    "        super(ViT_LSTM_Attention, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # --- 1. ENCODER (ViT) ---\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.vit_dim = 768  # Output dimension của ViT Base\n",
    "        \n",
    "        # Freeze & Unfreeze logic\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "        if unfreeze_layers > 0:\n",
    "            for layer in self.vit.encoder.layer[-unfreeze_layers:]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            for param in self.vit.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # --- 2. DECODER (LSTM + Attention) ---\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = Attention(encoder_dim=self.vit_dim, decoder_dim=hidden_dim, attention_dim=attention_dim)\n",
    "        \n",
    "        # Khởi tạo trạng thái hidden/cell của LSTM từ đặc trưng ảnh\n",
    "        self.init_h = nn.Linear(self.vit_dim, hidden_dim)\n",
    "        self.init_c = nn.Linear(self.vit_dim, hidden_dim)\n",
    "        \n",
    "        # LSTM Cell: Input = Embedding + Context Vector (từ Attention)\n",
    "        self.lstm_cell = nn.LSTMCell(embed_dim + self.vit_dim, hidden_dim)\n",
    "        \n",
    "        # Lớp đầu ra\n",
    "        self.f_beta = nn.Linear(hidden_dim, self.vit_dim)  # Gating sigmoid (tùy chọn)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        images: (Batch, 3, 224, 224)\n",
    "        captions: (Batch, Seq_Len) - Chứa cả <bos> và <eos>\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # 1. Encode ảnh\n",
    "        with torch.no_grad(): # Nếu muốn tiết kiệm mem, hoặc bỏ no_grad nếu finetune sâu\n",
    "            vit_output = self.vit(pixel_values=images).last_hidden_state # (B, 197, 768)\n",
    "        \n",
    "        encoder_out = vit_output # Giữ nguyên để attention soi vào 197 patches\n",
    "        \n",
    "        # 2. Khởi tạo LSTM state từ trung bình ảnh\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (B, hidden_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        \n",
    "        # 3. Chuẩn bị Teacher Forcing\n",
    "        # Input: <bos> ... từ_cuối (bỏ <eos> hoặc padding cuối)\n",
    "        # Length thực tế cần chạy = seq_len - 1\n",
    "        embeddings = self.embedding(captions) # (B, Seq_Len, Embed_Dim)\n",
    "        \n",
    "        # Tensor chứa kết quả: (B, Seq_Len-1, Vocab)\n",
    "        # Lưu ý: shape output phải khớp với targets (captions[:, 1:])\n",
    "        seq_len = captions.size(1) - 1 \n",
    "        predictions = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n",
    "        \n",
    "        # 4. Vòng lặp LSTM từng bước thời gian\n",
    "        for t in range(seq_len):\n",
    "            # Lấy attention context từ hidden state cũ\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "            \n",
    "            # Gating (mẹo nhỏ giúp train ổn định hơn, tuỳ chọn)\n",
    "            gate = torch.sigmoid(self.f_beta(h))\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            \n",
    "            # Input cho LSTM step t: Embedding từ t + Context\n",
    "            lstm_input = torch.cat([embeddings[:, t, :], attention_weighted_encoding], dim=1)\n",
    "            \n",
    "            # Update LSTM\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            \n",
    "            # Dự đoán từ tiếp theo\n",
    "            preds = self.fc_out(self.dropout(h))\n",
    "            predictions[:, t, :] = preds\n",
    "            \n",
    "        return predictions\n",
    "\n",
    "    def generate(self, images, bos_idx, eos_idx, max_len=40):\n",
    "        \"\"\"Hàm sinh caption (Greedy Search)\"\"\"\n",
    "        self.eval()\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vit_output = self.vit(pixel_values=images).last_hidden_state\n",
    "            encoder_out = vit_output\n",
    "            mean_encoder_out = encoder_out.mean(dim=1)\n",
    "            h = self.init_h(mean_encoder_out)\n",
    "            c = self.init_c(mean_encoder_out)\n",
    "            \n",
    "            # Bắt đầu với <bos>\n",
    "            inputs = torch.tensor([bos_idx] * batch_size, dtype=torch.long).to(device) # (B,)\n",
    "            generated_ids = []\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                embeddings = self.embedding(inputs) # (B, Embed_Dim)\n",
    "                \n",
    "                attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
    "                gate = torch.sigmoid(self.f_beta(h))\n",
    "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "                \n",
    "                lstm_input = torch.cat([embeddings, attention_weighted_encoding], dim=1)\n",
    "                \n",
    "                h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                preds = self.fc_out(h) # (B, Vocab)\n",
    "                \n",
    "                predicted_id = preds.argmax(dim=1) # (B,)\n",
    "                generated_ids.append(predicted_id.unsqueeze(1))\n",
    "                \n",
    "                # Update input cho bước sau\n",
    "                inputs = predicted_id\n",
    "                \n",
    "                # (Tùy chọn) Break nếu tất cả batch đều ra EOS (code đơn giản bỏ qua)\n",
    "\n",
    "            generated_ids = torch.cat(generated_ids, dim=1) # (B, Max_Len)\n",
    "            return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from process_data import create_dataloaders\n",
    "\n",
    "\n",
    "# Setup paths\n",
    "os.makedirs(\"model_ver5\", exist_ok=True)\n",
    "CSV_PATH = \"data/final_combined_ds_tokenized.csv\"\n",
    "VOCAB_PATH = \"data/vocab_vi_underthesea.json\"\n",
    "DATA_ROOT = \"data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE: {device}\")\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader, train_ds, val_ds, test_ds = create_dataloaders(\n",
    "    csv_path=CSV_PATH,\n",
    "    vocab_path=VOCAB_PATH,\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "vocab_size = len(train_ds.stoi)\n",
    "pad_idx = train_ds.pad_idx\n",
    "bos_idx = train_ds.bos_idx\n",
    "eos_idx = train_ds.eos_idx\n",
    "print(f\"\\nDATASET INFO:\")\n",
    "print(f\"  Vocab size: {vocab_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Pad/BOS/EOS: {pad_idx}/{bos_idx}/{eos_idx}\\n\")\n",
    "\n",
    "\n",
    "# Khởi tạo Model\n",
    "model = ViT_LSTM_Attention(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_idx=pad_idx,\n",
    "    embed_dim=512,\n",
    "    hidden_dim=512,\n",
    "    attention_dim=256,\n",
    "    unfreeze_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# 3. Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.1)\n",
    "\n",
    "vit_params = list(map(id, model.vit.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in vit_params, model.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.vit.parameters(), 'lr': 1e-5}, # Encoder học chậm\n",
    "    {'params': base_params, 'lr': 4e-4}             # Decoder LSTM cần LR cao hơn Transformer chút (thường 4e-4 hoặc 5e-4)\n",
    "], weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "PATIENCE = 4\n",
    "patience_counter = 0\n",
    "EPOCHS = 20\n",
    "VOCAB_SIZE = vocab_size\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # TRAINING\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for images, captions, lengths in progress_bar:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Với code model mới:\n",
    "        # Input vào model là 'captions' đầy đủ (để model tự lấy embedding từng bước)\n",
    "        # Nhưng target để tính loss là từ từ thứ 2 trở đi (bỏ <bos>)\n",
    "        targets = captions[:, 1:] \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # Model ViT_LSTM_Attention của chúng ta đã viết sẵn vòng lặp bên trong\n",
    "        # Nó sẽ trả về predictions cho (seq_len - 1) bước\n",
    "        outputs = model(images, captions) \n",
    "        \n",
    "        # Flatten để tính Loss\n",
    "        loss = criterion(outputs.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping rất quan trọng với LSTM để tránh bùng nổ gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = captions[:, 1:]\n",
    "            \n",
    "            outputs = model(images, captions)\n",
    "            \n",
    "            loss = criterion(outputs.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # CHECKPOINTING\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save(model.state_dict(), 'model_ver5/best_model.pth')\n",
    "        print(f\"  ✓ Best model saved! Val Loss: {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ⚠ No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs!\")\n",
    "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "print(\"Hoàn tất huấn luyện!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import generate_captions_for_dataset, compute_bleu_meteor, compute_cider\n",
    "import json\n",
    "\n",
    "BEST_MODEL_PATH = \"model_5/best_model.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "\n",
    "test_refs, test_hyps = generate_captions_for_dataset(\n",
    "    model,\n",
    "    test_ds,\n",
    "    device,\n",
    "    bos_idx=bos_idx,\n",
    "    eos_idx=eos_idx,\n",
    "    max_len=40,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "metrics = compute_bleu_meteor(test_refs, test_hyps)\n",
    "\n",
    "try:\n",
    "    cider_score = compute_cider(test_refs, test_hyps)\n",
    "    metrics[\"CIDEr\"] = cider_score\n",
    "except Exception as e:\n",
    "    print(\"Không tính được CIDEr, lỗi:\", e)\n",
    "\n",
    "print(\"ViT + Transformer:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "output_caption_file = \"model_5/vit_transformer_generated_captions.json\"\n",
    "save_captions = {k: {\"pred\": v, \"ref\": test_refs.get(k, [])} for k, v in test_hyps.items()}\n",
    "with open(output_caption_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(save_captions, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
