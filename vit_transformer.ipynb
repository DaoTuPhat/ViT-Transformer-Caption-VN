{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
      "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Collecting pycocoevalcap\n",
      "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap) (2.0.10)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
      "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "\n",
    "!pip install nltk\n",
    "\n",
    "!python -m nltk.downloader punkt\n",
    "\n",
    "!pip install pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Miniconda\\envs\\deep_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import ViTModel\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from process_data import create_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_VN_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, num_heads=8, num_layers=4, max_len=40, unfreeze_layers=2):\n",
    "        super(ViT_VN_Transformer, self).__init__()\n",
    "        \n",
    "        # ENCODER: Pre-trained ViT (Google)\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        \n",
    "        # Đóng băng (Freeze) toàn bộ\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if unfreeze_layers > 0:\n",
    "            layers_to_train = self.vit.encoder.layer[-unfreeze_layers:]\n",
    "            \n",
    "            for layer in layers_to_train:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "            \n",
    "            for param in self.vit.layernorm.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Kích thước đầu ra của ViT Base là 768\n",
    "        self.vit_hidden_size = 768\n",
    "        \n",
    "\n",
    "        # Cầu nối (Bridge): Chuyển từ 768 (ViT) -> 512 (Decoder)\n",
    "        self.feature_proj = nn.Linear(self.vit_hidden_size, embed_dim)\n",
    "        \n",
    "        \n",
    "        # --- 2. DECODER: Transformer thuần ---\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Embedding chữ: Biến ID số thành vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional Encoding: Giúp model biết thứ tự từ (trước/sau)\n",
    "        # Ở đây dùng Learnable Positional Embedding cho đơn giản và hiệu quả\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
    "        \n",
    "        # Khối Transformer Decoder\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True, # Quan trọng: input shape là (Batch, Seq, Dim)\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Lớp đầu ra: Biến vector 512 thành xác suất của từng từ trong từ điển\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def get_tgt_mask(self, size):\n",
    "        # Tạo mask tam giác để che tương lai\n",
    "        # Model không được nhìn thấy từ thứ 2 khi đang đoán từ thứ 1\n",
    "        mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        Hàm này dùng để TRAINING\n",
    "        images: (Batch, 3, 224, 224)\n",
    "        captions: (Batch, Seq_Len) - Bao gồm cả <bos> và <eos>\n",
    "        \"\"\"\n",
    "        device = images.device\n",
    "        \n",
    "        # --- A. Encode Ảnh ---\n",
    "        vit_output = self.vit(pixel_values=images).last_hidden_state\n",
    "        visual_features = self.feature_proj(vit_output)\n",
    "        \n",
    "        # --- B. Embed Text ---\n",
    "        # captions shape: (Batch, Seq_Len)\n",
    "        seq_len = captions.size(1)\n",
    "        \n",
    "        # Biến chữ thành vector + Cộng vị trí\n",
    "        tgt_emb = self.embedding(captions) * math.sqrt(self.embed_dim)\n",
    "        tgt_emb = tgt_emb + self.pos_embedding[:, :seq_len, :]\n",
    "        tgt_emb = self.dropout(tgt_emb)\n",
    "        \n",
    "        # --- C. Masking ---\n",
    "        # Tạo mask che tương lai (Causal Mask)\n",
    "        tgt_mask = self.get_tgt_mask(seq_len).to(device)\n",
    "        \n",
    "        # Padding Mask (để model không quan tâm đến số 0 ở cuối câu)\n",
    "        # Giả sử pad_idx = 0. Tạo mask True ở nơi có padding\n",
    "        tgt_padding_mask = (captions == 0).to(device) \n",
    "        \n",
    "        # --- D. Decode ---\n",
    "        # memory: là visual_features\n",
    "        # tgt: là caption embeddings\n",
    "        output = self.decoder(\n",
    "            tgt=tgt_emb, \n",
    "            memory=visual_features, \n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=None # ViT không có padding\n",
    "        )\n",
    "        \n",
    "        # --- E. Dự đoán ---\n",
    "        # Output: (Batch, Seq_Len, Vocab_Size)\n",
    "        prediction = self.fc_out(output)\n",
    "        \n",
    "        return prediction\n",
    "\n",
    "    def generate(self, images, bos_idx, eos_idx, max_len=40):\n",
    "        \"\"\"\n",
    "        Hàm này dùng để SUY LUẬN (INFERENCE/TEST)\n",
    "        Chạy vòng lặp sinh từng từ một.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # 1. Encode ảnh (Chỉ làm 1 lần)\n",
    "        with torch.no_grad():\n",
    "            vit_output = self.vit(pixel_values=images).last_hidden_state\n",
    "            visual_features = self.feature_proj(vit_output) # (B, 197, 512)\n",
    "            \n",
    "        # 2. Khởi tạo câu bắt đầu bằng <bos>\n",
    "        # Input hiện tại: [BOS]\n",
    "        generated = torch.full((batch_size, 1), bos_idx, dtype=torch.long).to(device)\n",
    "        \n",
    "        # 3. Vòng lặp sinh từ\n",
    "        for _ in range(max_len):\n",
    "            seq_len = generated.size(1)\n",
    "            \n",
    "            # Embed input hiện tại\n",
    "            tgt_emb = self.embedding(generated) * math.sqrt(self.embed_dim)\n",
    "            tgt_emb = tgt_emb + self.pos_embedding[:, :seq_len, :]\n",
    "            \n",
    "            # Đưa vào Decoder\n",
    "            # Lưu ý: Lúc generate ta không cần mask che tương lai vì ta chưa có tương lai\n",
    "            output = self.decoder(tgt=tgt_emb, memory=visual_features)\n",
    "            \n",
    "            # Lấy output của từ cuối cùng\n",
    "            last_token_output = output[:, -1, :] # (Batch, Dim)\n",
    "            \n",
    "            # Dự đoán từ tiếp theo (Logits -> Argmax)\n",
    "            logits = self.fc_out(last_token_output)\n",
    "            next_token = logits.argmax(dim=-1).unsqueeze(1) # (Batch, 1)\n",
    "            \n",
    "            # Nối vào câu\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "            \n",
    "            # (Tùy chọn) Nếu muốn tối ưu tốc độ:\n",
    "            # Kiểm tra xem tất cả các mẫu trong batch đã gặp <eos> chưa để break sớm\n",
    "            # Nhưng để đơn giản code thì cứ chạy hết max_len cũng được.\n",
    "            \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "CSV_PATH = \"data/final_combined_ds_tokenized.csv\"\n",
    "VOCAB_PATH = \"data/vocab_vi_underthesea.json\"\n",
    "DATA_ROOT = \"data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE: {device}\")\n",
    "\n",
    "os.makedirs(\"model_ver5\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader, test_loader, train_ds, val_ds, test_ds = create_dataloaders(\n",
    "    csv_path=CSV_PATH,\n",
    "    vocab_path=VOCAB_PATH,\n",
    "    data_root=DATA_ROOT,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "vocab_size = len(train_ds.stoi)\n",
    "pad_idx = train_ds.pad_idx\n",
    "bos_idx = train_ds.bos_idx\n",
    "eos_idx = train_ds.eos_idx\n",
    "print(f\"\\nDATASET INFO:\")\n",
    "print(f\"  Vocab size: {vocab_size}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Pad/BOS/EOS: {pad_idx}/{bos_idx}/{eos_idx}\\n\")\n",
    "\n",
    "\n",
    "# Cấu hình Hyperparameters\n",
    "VOCAB_SIZE = len(train_ds.stoi) # Lấy từ dataset đã tạo\n",
    "EMBED_DIM = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4                  # Model nhỏ gọn cho 30k ảnh\n",
    "UNFREEZE_LAYERS = 2             # Mở khóa 2 lớp cuối của ViT\n",
    "EPOCHS = 20                     # Train tiếng Việt từ đầu cần nhiều epoch hơn fine-tune\n",
    "\n",
    "\n",
    "# Khởi tạo Model\n",
    "model = ViT_VN_Transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    unfreeze_layers=UNFREEZE_LAYERS,\n",
    "    max_len=40\n",
    ").to(device)\n",
    "\n",
    "# 3. Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_ds.pad_idx, label_smoothing=0.1)\n",
    "vit_params = list(map(id, model.vit.parameters()))\n",
    "base_params = filter(lambda p: id(p) not in vit_params, model.parameters())\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.vit.parameters(), 'lr': 1e-5},  # Learning rate nhỏ cho Encoder\n",
    "    {'params': base_params, 'lr': 1e-4}              # Learning rate chuẩn cho Decoder\n",
    "], weight_decay=1e-4) # Thêm weight_decay để giảm overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "PATIENCE = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # TRAINING\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for images, captions, lengths in progress_bar:\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Input cho model: Bỏ từ cuối cùng (<eos> hoặc padding cuối)\n",
    "        # Target (Đáp án): Bỏ từ đầu tiên (<bos>)\n",
    "        decoder_input = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images, decoder_input) \n",
    "        # Output shape: (Batch, Seq_Len, Vocab)\n",
    "        \n",
    "        # Tính Loss\n",
    "        # Flatten dữ liệu để tính CrossEntropy: (Batch * Seq_Len, Vocab)\n",
    "        loss = criterion(outputs.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            decoder_input = captions[:, :-1]\n",
    "            targets = captions[:, 1:]\n",
    "            \n",
    "            outputs = model(images, decoder_input)\n",
    "            loss = criterion(outputs.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # CHECKPOINTING\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save(model.state_dict(), 'model_ver5/best_model.pth')\n",
    "        print(f\"  ✓ Best model saved! Val Loss: {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  ⚠ No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    # EARLY STOPPING\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\n⚠ Early stopping triggered after {epoch+1} epochs!\")\n",
    "        print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "print(\"Hoàn tất huấn luyện!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import generate_captions_for_dataset, compute_bleu_meteor, compute_cider\n",
    "import json\n",
    "\n",
    "BEST_MODEL_PATH = \"model_5/best_model.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "\n",
    "test_refs, test_hyps = generate_captions_for_dataset(\n",
    "    model,\n",
    "    test_ds,\n",
    "    device,\n",
    "    bos_idx=bos_idx,\n",
    "    eos_idx=eos_idx,\n",
    "    max_len=40,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "metrics = compute_bleu_meteor(test_refs, test_hyps)\n",
    "\n",
    "try:\n",
    "    cider_score = compute_cider(test_refs, test_hyps)\n",
    "    metrics[\"CIDEr\"] = cider_score\n",
    "except Exception as e:\n",
    "    print(\"Không tính được CIDEr, lỗi:\", e)\n",
    "\n",
    "print(\"ViT + Transformer:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "output_caption_file = \"model_5/vit_transformer_generated_captions.json\"\n",
    "save_captions = {k: {\"pred\": v, \"ref\": test_refs.get(k, [])} for k, v in test_hyps.items()}\n",
    "with open(output_caption_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(save_captions, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu train và val losses vào file để vẽ đồ thị sau\n",
    "with open(\"model_5/train_val_losses.txt\", \"w\") as f:\n",
    "    f.write(\"Train Losses:\\n\")\n",
    "    f.write(\",\".join([str(loss) for loss in train_losses]) + \"\\n\")\n",
    "    f.write(\"Val Losses:\\n\")\n",
    "    f.write(\",\".join([str(loss) for loss in val_losses]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
